{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3789e0a8",
   "metadata": {},
   "source": [
    "# 📘 Introduction\n",
    "This notebook covers the full pipeline for collecting, cleaning, and preparing stock + earnings event data.\n",
    "The output is a cleaned dataset (`strategy_df`) used for modeling and strategy simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185315f8",
   "metadata": {},
   "source": [
    "## 📦 Data Collection\n",
    "We begin by collecting three main datasets:\n",
    "- `cleaned_stock_data`: Historical stock price data (daily OHLCV)\n",
    "- `combined_alpha_earnings_data`: Earnings report announcements\n",
    "- `ticker_df`: Metadata and fundamentals (sector, employees, CEO, etc.)\n",
    "\n",
    "**Key Merge Strategy:**\n",
    "- Merge on `Ticker` and `reported_date_clean` (earnings date)\n",
    "- Dates are converted to `datetime64` for proper filtering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d7f941",
   "metadata": {},
   "source": [
    "## Data Preprocessing Pipeline\n",
    "For each earnings event, we extract a window of ±N days.\n",
    "- `Days_From_Earnings`: Offset in days from the earnings announcement\n",
    "- `Next_Day_Open`: Next day's open price (used for after-hours return)\n",
    "- `Regular_Change%`: Day change from Open to Close\n",
    "- `After_Hours_Change%`: Change from Close to Next Day Open\n",
    "\n",
    "EPS metrics from `earnings_df` are also injected into the window."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dab0fbc",
   "metadata": {},
   "source": [
    "## 🧠 Feature Engineering & Final Columns\n",
    "Only safe (non-leaky) columns are retained for modeling. These include:\n",
    "- Price-based returns: `Regular_Change%`, `After_Hours_Change%`\n",
    "- EPS details: `EPS_Actual`, `EPS_Estimate`, `EPS_Surprise`, `EPS_Surprise_%`\n",
    "- Context: `Days_From_Earnings`, `Sector`, `Profitable`, `FemaleCEO`, etc.\n",
    "\n",
    "All rows with missing critical values are dropped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c49595b",
   "metadata": {},
   "source": [
    "## 💾 Export\n",
    "The result is saved as `strategy_df_window7.csv` and used in the modeling phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mxu9aGU__4He"
   },
   "source": [
    "This notebook covers the full pipeline for collecting, cleaning, and preparing stock + earnings event data. The output of this notebook is a cleaned dataset called `strategy_df`, which will be used for modeling and simulation in subsequent notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XS3Ph7xex2nO"
   },
   "source": [
    "Loading the kaggle auth json into the colab session.\n",
    "Once you login into kaggle, this can be download from the logged in session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QufBkxErx-Wx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "for fn in uploaded.keys():\n",
    "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
    "      name=fn, length=len(uploaded[fn])))\n",
    "\n",
    "# Print the current working directory\n",
    "print(\"\\nCurrent working directory:\")\n",
    "print(os.getcwd())\n",
    "\n",
    "# Then move kaggle.json into the folder where the API expects to find it.\n",
    "!mkdir -p ~/.kaggle\n",
    "!cp kaggle.json ~/.kaggle/\n",
    "!chmod 600 ~/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tgck4HpNyMng"
   },
   "source": [
    "# DataSet 1: Ticker Information (ticker_df):\n",
    "Download the fortune 1000 company information from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UshRVVmyyVpG"
   },
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import os\n",
    "\n",
    "dataset_name = 'jeannicolasduval/2024-fortune-1000-companies'\n",
    "\n",
    "kaggle.api.dataset_download_files(dataset_name, path='.', unzip=True)\n",
    "\n",
    "download_path = 'ticker/'\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63nNK-C0ygfX"
   },
   "source": [
    "Loading the data into a data frame ticker_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxFG-QLsyopv"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "file_path = './ticker/fortune1000_2024.csv'\n",
    "if os.path.exists(file_path):\n",
    "    try:\n",
    "        ticker_df = pd.read_csv(file_path, index_col=0)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found. Double-check the path.\")\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Error: File '{file_path}' is empty.\")\n",
    "    except pd.errors.ParserError:\n",
    "        print(f\"Error: Problem parsing '{file_path}'. Check for incorrect delimiters or data types.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "else:\n",
    "   print(f\"Error: File '{file_path}' not found. Make sure the filename and path are correct.\")\n",
    "   ticker_df = None\n",
    "\n",
    "\n",
    "ticker_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZpHsDZp1s2O"
   },
   "source": [
    "# DataSet 2: Get earning report dates from Alpha Vantage (combined_alpha_earnings_data):\n",
    "Invoking the alpha vantage earnings API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AKYeffiP2RjI"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "api_key = \"8Y236L6QW3EKZD6K\"\n",
    "\n",
    "def get_earnings_data(ticker):\n",
    "    \"\"\"Fetch earnings report dates from Alpha Vantage\"\"\"\n",
    "    url = f\"https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={api_key}\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data for {ticker}: {e}\")\n",
    "        return pd.DataFrame()  # Return an empty DataFrame on error\n",
    "\n",
    "    earnings_list = []\n",
    "\n",
    "    if \"quarterlyEarnings\" in data:\n",
    "        for entry in data[\"quarterlyEarnings\"]:\n",
    "            try:\n",
    "                earnings_list.append({\n",
    "                    \"symbol\": ticker,\n",
    "                    \"reported_date\": pd.to_datetime(entry[\"reportedDate\"]),\n",
    "                    \"actual_eps\": float(entry[\"reportedEPS\"]) if entry[\"reportedEPS\"] != \"None\" else None,\n",
    "                    \"estimated_eps\": float(entry[\"estimatedEPS\"]) if entry[\"estimatedEPS\"] != \"None\" else None,\n",
    "                    \"surprise\": float(entry[\"surprise\"]) if entry[\"surprise\"] != \"None\" else None,\n",
    "                    \"surprise_pct\": float(entry[\"surprisePercentage\"]) if entry[\"surprisePercentage\"] != \"None\" else None,\n",
    "                })\n",
    "            except KeyError as e:\n",
    "                print(f\"KeyError in earnings data for {ticker}: {e}\")\n",
    "                continue\n",
    "\n",
    "    print(f\"Loaded {len(earnings_list)} earnings reports for {ticker}\")\n",
    "\n",
    "    return pd.DataFrame(earnings_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JwNh4ha2oRy"
   },
   "source": [
    "Since the API was rate-limited. Pulled the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LyNbW_8t2Wex"
   },
   "outputs": [],
   "source": [
    "# Function to gather earnings data for all tickers using your existing function\n",
    "def get_all_earnings_data(tickers, get_earnings_data_fn):\n",
    "    all_earnings = []\n",
    "    for ticker in tickers:\n",
    "        try:\n",
    "            df = get_earnings_data_fn(ticker)\n",
    "            if not df.empty:\n",
    "                all_earnings.append(df)\n",
    "                print(f\"Fetched earnings for {ticker}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed for {ticker}: {e}\")\n",
    "    return pd.concat(all_earnings, ignore_index=True)\n",
    "\n",
    "# Run this for a small subset first to test (e.g., first 10 tickers)\n",
    "earnings_data_top10 = get_all_earnings_data(top_10_tickers, get_earnings_data)\n",
    "earnings_data_top10.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yxv0svwJ4B_W"
   },
   "outputs": [],
   "source": [
    "api_key = \"8Y236L6QW3EKZD6K\"\n",
    "\n",
    "# These are the 24 tickers still needed (after removing the Top 10 already fetched)\n",
    "tickers_missing = [\n",
    "    'PSX', 'CI', 'MPC', 'HD', 'GOOG', 'XOM', 'MCK', 'CNC', 'GM', 'CVX', 'C',\n",
    "    'UNH', 'BRK.A', 'F', 'CVS', 'BAC', 'COST', 'WBA', 'FB', 'KR', 'CAH',\n",
    "    'ABC', 'WMT', 'ELV', 'VLO'\n",
    "]\n",
    "\n",
    "# Fetch only these (add delay=13 to avoid rate limit per minute)\n",
    "earnings_data_remaining = get_all_earnings_data(tickers_missing, get_earnings_data, delay=13)\n",
    "\n",
    "# Combine with the existing top 10 data\n",
    "earnings_data_top30 = pd.concat([earnings_data_top10, earnings_data_remaining], ignore_index=True)\n",
    "\n",
    "# Check result\n",
    "print(\"✅ Total earnings entries (Top 30):\", len(earnings_data_top30))\n",
    "earnings_data_top30['symbol'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EM4Sgo0m4Mtm"
   },
   "outputs": [],
   "source": [
    "tickers_to_retry = ['META', 'WMT', 'ELV']\n",
    "\n",
    "earnings_retry = get_all_earnings_data(tickers_to_retry, get_earnings_data, delay=13)\n",
    "\n",
    "earnings_data_top30 = pd.concat([earnings_data_top30, earnings_retry], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DNKYxPMn7EQR"
   },
   "outputs": [],
   "source": [
    "# Combine both earnings datasets into one\n",
    "earnings_data_full = pd.concat([earnings_data_top30, earnings_data_remaining], ignore_index=True)\n",
    "\n",
    "# Optional: Drop duplicates just in case (some overlaps could exist)\n",
    "earnings_data_full.drop_duplicates(subset=['symbol', 'reported_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gzv7jt-N7KaQ"
   },
   "outputs": [],
   "source": [
    "# Convert event-based top30 earnings to the same format\n",
    "alpha_format_top30 = earnings_data_top30[[\n",
    "    'Ticker', 'Earnings_Date', 'EPS_Actual', 'EPS_Estimate', 'EPS_Surprise', 'EPS_Surprise_%'\n",
    "]].drop_duplicates()\n",
    "\n",
    "# Rename columns to match the earnings_data_remaining format\n",
    "alpha_format_top30 = alpha_format_top30.rename(columns={\n",
    "    'Ticker': 'symbol',\n",
    "    'Earnings_Date': 'reported_date',\n",
    "    'EPS_Actual': 'actual_eps',\n",
    "    'EPS_Estimate': 'estimated_eps',\n",
    "    'EPS_Surprise': 'surprise',\n",
    "    'EPS_Surprise_%': 'surprise_pct'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuShuPRx7O96"
   },
   "outputs": [],
   "source": [
    "# Combine the two\n",
    "all_earnings_alpha = pd.concat([alpha_format_top30, earnings_data_remaining], ignore_index=True)\n",
    "\n",
    "# Drop any exact duplicates (some may overlap)\n",
    "all_earnings_alpha = all_earnings_alpha.drop_duplicates(subset=['symbol', 'reported_date'])\n",
    "\n",
    "# Check how many unique tickers we now have\n",
    "print(\"🧠 Total earnings records:\", len(all_earnings_alpha))\n",
    "print(\"🔢 Unique tickers with Alpha Vantage data:\", all_earnings_alpha['symbol'].nunique())\n",
    "print(\"🧾 Example tickers:\", all_earnings_alpha['symbol'].unique()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZgiIYh3f7WBM"
   },
   "outputs": [],
   "source": [
    "Taking a back up to ensure the data can be reloaded on a different day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noB3SAgG7SSq"
   },
   "outputs": [],
   "source": [
    "# STEP 1: Save the combined earnings data to a CSV\n",
    "all_earnings_alpha.to_csv(\"combined_alpha_earnings_data.csv\", index=False)\n",
    "\n",
    "# STEP 2: Create a download link (works in Colab)\n",
    "from google.colab import files\n",
    "files.download(\"combined_alpha_earnings_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qhz1aIeQ7quf"
   },
   "source": [
    "Reload the data on a different day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7e9w86hO7pgn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Adjust the path based on where your file is stored in Drive\n",
    "file_path = '/content/combined_alpha_earnings_data (1).csv'\n",
    "combined_alpha_earnings_data = pd.read_csv(file_path)\n",
    "\n",
    "# Preview\n",
    "combined_alpha_earnings_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvRu1L1uy94U"
   },
   "source": [
    "# DataSet 3: Stock Daily trade Information (combined_stock_data):\n",
    "Bulk data set of stock ticker information until december 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JqO2abPkzoI7"
   },
   "outputs": [],
   "source": [
    "import kaggle\n",
    "import os\n",
    "\n",
    "dataset_name = 'paultimothymooney/stock-market-data'\n",
    "\n",
    "kaggle.api.dataset_download_files(dataset_name, path='.', unzip=True)\n",
    "\n",
    "download_path = 'data/'\n",
    "if not os.path.exists(download_path):\n",
    "    os.makedirs(download_path)\n",
    "kaggle.api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GGbWeRln0OXX"
   },
   "source": [
    "Only loading tickers present in ticker_df in order to simplify the bulk data set that is present in this kaggle set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G678QDHA0LU7"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def add_ticker_column(df, filename, index):\n",
    "    \"\"\"Adds a ticker column to the DataFrame based on the filename.\"\"\"\n",
    "    ticker = filename.split('.')[0]  # Extract ticker from filename\n",
    "    df['Ticker'] = ticker\n",
    "    df['Index'] = index\n",
    "    return df\n",
    "\n",
    "def load_and_combine_csv_data(base_dir, ticker_df):\n",
    "    \"\"\"\n",
    "    Loads CSV files only for tickers present in ticker_df, adds a Ticker column,\n",
    "    and combines them into a single DataFrame.\n",
    "    \"\"\"\n",
    "    if ticker_df is None:\n",
    "        print(\"Error: ticker_df is None.  Ensure it's loaded before calling this function.\")\n",
    "        return None\n",
    "\n",
    "    valid_tickers = ticker_df['Ticker'].str.upper().tolist()  # Get list of valid tickers, convert to uppercase\n",
    "\n",
    "    all_data = []\n",
    "    for subdir in os.listdir(base_dir): # go through base, where stock market dir is\n",
    "        subdir_path = os.path.join(base_dir, subdir)\n",
    "        if os.path.isdir(subdir_path): # If is dir\n",
    "\n",
    "            csv_dir = os.path.join(subdir_path, \"csv\") # go to the csv dir inside the base\n",
    "            if os.path.exists(csv_dir):\n",
    "                for filename in os.listdir(csv_dir):\n",
    "                    if filename.endswith(\".csv\"):\n",
    "                        ticker = filename.split('.')[0].upper()  # Extract ticker and convert to uppercase\n",
    "\n",
    "                        #if ticker in valid_tickers:  # Check if ticker is in the valid list\n",
    "                        csv_path = os.path.join(csv_dir, filename)\n",
    "                        try:\n",
    "                            df = pd.read_csv(csv_path, parse_dates=['Date'])  # Parse date\n",
    "\n",
    "                            # Date is not defined, print so we know\n",
    "                            if 'Date' not in df:\n",
    "                                print(f\"Issue, Date column not defined in {filename}\")\n",
    "\n",
    "                            df = add_ticker_column(df, filename, subdir_path)  # ticker to df from file name\n",
    "                            all_data.append(df)  # Append the df to all_data\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error loading or processing {filename}: {e}\")\n",
    "                        # else:\n",
    "                        #     print(f\"Skipping {filename}: Ticker not found in ticker_df.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"csv directory not found in {subdir}\")\n",
    "\n",
    "    if not all_data:\n",
    "        print(\"No data loaded. Ensure the base directory contains the specified tickers, and the CSV files are correctly named.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)  # combine\n",
    "    return combined_df\n",
    "\n",
    "# --- Main ---\n",
    "base_dir = \"/content/stock_market_data/\"  # The main directory with the four folders\n",
    "\n",
    "# ASSUMPTION: ticker_df is already loaded and accessible in this scope!\n",
    "\n",
    "combined_stock_data = load_and_combine_csv_data(base_dir, ticker_df)  # Loads data for tickers present in ticker_df.  Pass ticker_df to the function\n",
    "\n",
    "if combined_stock_data is not None:\n",
    "    print(f\"Combined data shape: {combined_stock_data.shape}\")\n",
    "    print(f\"Combined data columns: {combined_stock_data.columns}\")\n",
    "    print(combined_stock_data.head())  # Print the head of the data\n",
    "else:\n",
    "    print(\"Failed to load combined stock data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kP38k0Ar1lc0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wMLp5d_W1kYV"
   },
   "outputs": [],
   "source": [
    "def inspect_and_clean_ticker(df, ticker):\n",
    "    \"\"\"Clean and return stock data for a single ticker.\"\"\"\n",
    "    subset = df[df['Ticker'] == ticker].copy()\n",
    "    subset['Date'] = pd.to_datetime(subset['Date'], errors='coerce')\n",
    "    cleaned = subset.dropna(subset=['Date'])\n",
    "\n",
    "    if cleaned.empty:\n",
    "        print(f\"⚠️ No clean data found for {ticker}\")\n",
    "    else:\n",
    "        print(f\"✅ {ticker}: {cleaned.shape[0]} rows | Date range: {cleaned['Date'].min().date()} → {cleaned['Date'].max().date()}\")\n",
    "\n",
    "    return cleaned\n",
    "\n",
    "# ✅ Loop over all tickers present in the earnings dataset\n",
    "tickers_to_clean = all_earnings_alpha['symbol'].unique().tolist()\n",
    "cleaned_ticker_dfs = []\n",
    "\n",
    "for ticker in tickers_to_clean:\n",
    "    cleaned = inspect_and_clean_ticker(filtered_stock_df, ticker)\n",
    "    if not cleaned.empty:\n",
    "        cleaned_ticker_dfs.append(cleaned)\n",
    "\n",
    "# 📦 Combine all cleaned ticker data\n",
    "cleaned_stock_data = pd.concat(cleaned_ticker_dfs, ignore_index=True)\n",
    "\n",
    "print(f\"\\n📊 Final cleaned stock data shape: {cleaned_stock_data.shape}\")\n",
    "print(f\"🔢 Tickers included: {cleaned_stock_data['Ticker'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjVQK-sf_y6a"
   },
   "source": [
    "Preparing a final combined dataset required for processing for a specific window size this method can be run multiple times and used to extract the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HNa0PVVYAT1N"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def build_event_and_strategy_df(stock_df, earnings_df, ticker_df, window=5):\n",
    "    # Step 1: Clean & Convert Dates\n",
    "    stock_df[\"Date\"] = pd.to_datetime(stock_df[\"Date\"])\n",
    "    earnings_df[\"reported_date_clean\"] = pd.to_datetime(earnings_df[\"reported_date_clean\"])\n",
    "\n",
    "    # Step 2: Sort for merge\n",
    "    stock_df = stock_df.sort_values([\"Ticker\", \"Date\"])\n",
    "\n",
    "    # Step 3: Build event windows\n",
    "    event_rows = []\n",
    "    for (ticker, reported_date) in earnings_df[[\"symbol\", \"reported_date_clean\"]].dropna().values:\n",
    "        price_df = stock_df[stock_df[\"Ticker\"] == ticker]\n",
    "        idx = price_df[price_df[\"Date\"] == reported_date].index\n",
    "        if len(idx) == 0:\n",
    "            continue\n",
    "        loc = price_df.index.get_loc(idx[0])\n",
    "        start = max(0, loc - window)\n",
    "        end = min(len(price_df), loc + window + 1)\n",
    "        window_df = price_df.iloc[start:end].copy()\n",
    "        window_df[\"Days_From_Earnings\"] = np.arange(start - loc, end - loc)\n",
    "        window_df[\"Earnings_Date\"] = reported_date\n",
    "        window_df[\"symbol\"] = ticker\n",
    "        # Merge EPS info\n",
    "        for col in ['actual_eps', 'estimated_eps', 'surprise', 'surprise_pct']:\n",
    "            window_df[col] = earnings_df[(earnings_df[\"symbol\"] == ticker) &\n",
    "                                         (earnings_df[\"reported_date_clean\"] == reported_date)][col].values[0]\n",
    "        event_rows.append(window_df)\n",
    "\n",
    "    event_df = pd.concat(event_rows).reset_index(drop=True)\n",
    "\n",
    "    # Step 4: Add Next Day Open\n",
    "    event_df[\"Next_Day_Open\"] = event_df.groupby(\"Ticker\")[\"Open\"].shift(-1)\n",
    "\n",
    "    # Step 5: Add Return Metrics\n",
    "    event_df[\"Regular_Change%\"] = 100 * (event_df[\"Close\"] - event_df[\"Open\"]) / event_df[\"Open\"]\n",
    "    event_df[\"After_Hours_Change%\"] = 100 * (event_df[\"Next_Day_Open\"] - event_df[\"Close\"]) / event_df[\"Close\"]\n",
    "\n",
    "    # Step 6: Merge Ticker Metadata\n",
    "    enriched_df = pd.merge(event_df, ticker_df, how=\"left\", left_on=\"Ticker\", right_on=\"Ticker\")\n",
    "\n",
    "    # Rename EPS fields\n",
    "    enriched_df = enriched_df.rename(columns={\n",
    "        \"surprise\": \"EPS_Surprise\",\n",
    "        \"surprise_pct\": \"EPS_Surprise_%\",\n",
    "        \"actual_eps\": \"EPS_Actual\",\n",
    "        \"estimated_eps\": \"EPS_Estimate\"\n",
    "    })\n",
    "\n",
    "    # Reorder for modeling\n",
    "    strategy_cols = [\n",
    "        'Ticker', 'Earnings_Date', 'Date', 'Days_From_Earnings',\n",
    "        'Open', 'Close', 'Next_Day_Open',\n",
    "        'Regular_Change%', 'After_Hours_Change%', 'EPS_Actual', 'EPS_Estimate',\n",
    "        'EPS_Surprise', 'EPS_Surprise_%',\n",
    "        'Sector', 'Industry', 'Profitable', 'Founder_is_CEO',\n",
    "        'FemaleCEO', 'Growth_in_Jobs'\n",
    "    ]\n",
    "    strategy_df = enriched_df[strategy_cols].dropna()\n",
    "\n",
    "    return strategy_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XBoGxdeArXB"
   },
   "outputs": [],
   "source": [
    "strategy_df_1 = build_event_and_strategy_df(cleaned_stock_data, combined_alpha_earnings_data, ticker_df, window=1)\n",
    "strategy_df_1.to_csv(\"/content/strategy_df_window1.csv\", index=False)\n",
    "strategy_df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3JXEwvJqAvwS"
   },
   "outputs": [],
   "source": [
    "strategy_df_3 = build_event_and_strategy_df(cleaned_stock_data, combined_alpha_earnings_data, ticker_df, window=3)\n",
    "strategy_df_3.to_csv(\"/content/strategy_df_window3.csv\", index=False)\n",
    "strategy_df_3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FZh5H6Y8AgRH"
   },
   "outputs": [],
   "source": [
    "\n",
    "strategy_df_5 = build_event_and_strategy_df(cleaned_stock_data, combined_alpha_earnings_data, ticker_df, window=5)\n",
    "strategy_df_5.to_csv(\"/content/strategy_df_window5.csv\", index=False)\n",
    "strategy_df_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xR4smSigAyk-"
   },
   "outputs": [],
   "source": [
    "strategy_df_7 = build_event_and_strategy_df(cleaned_stock_data, combined_alpha_earnings_data, ticker_df, window=7)\n",
    "strategy_df_7.to_csv(\"/content/strategy_df_window7.csv\", index=False)\n",
    "strategy_df_7.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i31iM6U_A2ey"
   },
   "outputs": [],
   "source": [
    "strategy_df_9 = build_event_and_strategy_df(cleaned_stock_data, combined_alpha_earnings_data, ticker_df, window=9)\n",
    "strategy_df_9.to_csv(\"/content/strategy_df_window9.csv\", index=False)\n",
    "strategy_df_9.head()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
